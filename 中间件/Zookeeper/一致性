### Paxos算法简述
 Paxos不是一种算法，而是一种就多节点如何达成共识的指导性思想，其算法简言就是因为各个节点在写数据前先进行了共识，所以最终写入数据时就保证了数据的一致性。
 这里也说明了共识和一致性的区别，共识是过程，一致性是结果。反过来说，对于分布式事务，如果各个节点事前没有进行沟通达成共识，最终分布式数据可能就不会一致。
 当我们用的共识方案不同时，数据一致性的结果也会不同（例如严格一致性，顺序一致性）
 
### 分布式一致性有哪几种，分别使用了什么算法，举例有哪些中间件？
 1.严格一致性：在任意时刻各个节点的数据都完全一样，以paxos算法为基础的二阶段提交都保证了严格一致性，例如google的分布式锁服务chubby，分布式数据库spanner
 2.顺序一致性：任何一个读取返回新值后，所有后续读取（在相同或其他客户端上）也必须返回新值。顺序一致性是基于严格一致性和最终一致性的折中方案，也可以说是性能和可用性权宜之计。顺序一致性舍弃了严格一致性的2pc，而采用1pc效率更高。典型代表是raft算法和zab协议。
  【思考】哪种场景适用于顺序一致性？
 3.最终一致性：暂时不一致，但在未来某一时刻数据一定会一致。典型代表是gossip协议，采用流言广播+反熵机制保证最终一致性。比较有代表性的使用场景有分布式数据库cassandra以及RedisCluster。
 
### Paxos学习笔记

### Raft学习笔记
 1.工作流程-选举
   a) raft算法中有3个角色，Leader/Follower/Candicate，在初始状态每个节点都是Follower，且内部维护了一个随机计数器，当计数器跑完后会将自己晋升为Candicate，并将term加1
   b) 新晋升的Candicate向其他Follower发起投票，面对投票follower需要遵循"先到先得"和"只能投一票"的规定，且candicate的term一定要大于follower，否则follower是不会给投票的。
   c) 如果多个candicate选票一样，则重新触发选举流程；反之集群中会诞生出一名leader节点，其他节点角色都是follower，集群初始化完毕。
  [问题] 在选举时数据term会作为优先晋升leader的一个条件，数据版本呢？
 2.工作流程-复制
  [前提]首先要明确一点，关于raft写操作，只能由leader来执行，如果其他follower节点接到了写请求，也要将请求转发给leader。这也是raft算法一个弊端，写操作永远是单点。
   a) leader接收来自客户端/其他follower的写请求，保存数据，但此时数据因为没有同步到其他follower节点，所以状态是uncommit状态。
   b) leader向其他follower发送复制命令，将数据同步给其他节点。
   c) 当leader的数据复制超过半数的follower节点时，leader就认为写成功，此时将自己的数据状态变更为commit。
   d) leader返回给客户端成功的结果
   e) 当follower接收leader的心跳时，判断数据是否已提交，如果已经提交则自己的数据状态也更新。
  [总结] 由此可以看出，raft之所以没有像paxos那样实现严格一致性，终于是因为省略了2pc变成了1pc，但通过数据id连续性保证了数据顺序一致性(这点也和paxos不同，paxos不要求数据id是连续的)。
  [问题] 在第5步这里有一个问题，如果有个别follower在未接到心跳，此时数据已经和其他节点产生了差异，且客户端先读了接收心跳成功的follower节点，转而又有其他客户端过来丢失心跳follower的读请求，还能保证顺序一致性吗？
 3.raft如何解决脑裂问题
   多数派解决
 4.raft特点分析
  [优势] 一阶段提交，顺序一致性
  [弊端] leader单写；且集群强依赖leader，如果集群处于正在选举中，则对外读写服务都不可用。（可用性这么差，为什么zookeeper还能接受？基于什么场景考虑的？）
 相关学习资料：
   a) http://thesecretlivesofdata.com/raft/
   b) https://raft.github.io/raftscope/index.html
 
### Zab学习笔记

### Gossip学习笔记
 1.RedisCluster为什么要选用gossip协议；zookeeper为什么要选用zab协议。都是一致性协议两者是否可以互换？
   两者差异不是在于一致性上，而是对于可用性的要求不一样。
   1.首先RedisCluster是数据节点集群，如果RedisCluster实现raft协议，就代表整个集群只能有一个节点对外提供写操作，可用性太低；且Redis定位就是高可用，并要求使用者能够容忍短暂的数据不一致。
   2.etcd/zookeeper致力于解决服务注册和发现，因此基于场景而言，写流量就不会太大，只有一个leader来承载写流量足以，且对于依赖zk或etcd的rpc节点而言，是能够容忍短暂的leader重新选举，因为各个rpc节点内部会维护各自路由表信息，在相互调用时不依赖注册中心。
     其实我个人认为注册中心对一致性要求应该不用太高，且短时间内能保证最终一致性就可以。
 
### 兰伯特的 Multi-Paxos，虽然能保证达成共识后的值不再改变，但它不关心达成共识的值是什么，也无法保证各值（也就是操作）的顺序性。而这就是 Zookeeper 没有采用 Multi-Paxos 的原因，又是 ZAB 协议着力解决的
 是因为paxos有多个leader(提议者)吗？所以1，2，3请求到达集群后，可能根据leader执行速度不同，最终存储顺序是1，3，2
 
【问题1】如何保证传遍全局？
【问题2】如何保证消息传递时不会形成闭环？TTL保证？ 
 
 
 
#### NWR协议
 What：NWR协议是动态控制一致性级别的策略，这3个字母分别代表：N-副本数；W-写副本数；R-读副本数。当NWR设置不同值时，达到一致性效果也有所不同：
   1) W + R > N ：此时能实现强一致性
   2) W + R ≤ N ：此时实现最终一致性
 How：以强一致性N=5,W=4,R=2为例，即master有5个follower，当W=4时代表只有写入4个follower才算写成功；R=2代表读取时，至少要读取2个follower的数据，然后取出最新的版本返回给客户端。
   结论：虽然在写时可能只写入了4个节点，剩余1个没有写入成功，而Read时传入2代表要至少读2个Follower的数据，因此一定会读取到写入成功的Follower，最终将2个节点数据版本做对比返回，即实现了读写最终一致性。
 When：

 
一致性哈希虚拟节点的数量为什么要等于2的N次方



 
 
 
 而将paxos这种共识思想进行落地的前期有cubby和spanner，后来的简化版有raft和zab算法。基于这种算法的中间件都有一个共同的特点，对于leader角色尤为看重，如果集群中leader不存在时，整个集群是完全不可用的状态，读写均无法处理，例如etcd和zookeeper。这里可以看出来为了追求一致性而牺牲了可用性。



二阶段提交中，如果在第一次询问阶段ok，但follower立刻挂掉了，怎么保证二阶段的提交。

用什么方案达成acid和base
一般用2pc这种共识算法达成一致性。一般我们用异步补偿的方式实现最终一致性，除此之外像cassandra用读时修复或写时修复也可以。




raft在选举和写数据时，用到了共识算法。
你可以把 Raft 的日志复制理解成一个优化后的二阶段提交（将二阶段优化成了一阶段），减少了一半的往返消息，也就是降低了一半的消息延迟。领导者不直接发送消息通知其他节点应用指定日志项。因为领导者的日志复制 RPC 消息或心跳消息，包含了当前最大的，将会被提交（Commit）的日志项索引值。所以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。

cap的c其实是原子一致性，也叫线性一致性。一般说的强一致性是相对于最终一致性而言的，更高的级别是严格一致性，严格一致性是指在任意时刻各个节点的数据都完全一样。

什么场景下要保证顺序一致性，而最终一致性不满足呢。

raft通过二次提交实现的读是强一致性；
但是基于日志式的数据存储，读只能保证顺序一致性。

日志完整性高的跟随者（也就是最后一条日志项对应的任期编号值更大，索引号更大），拒绝投票给日志完整性低的候选人。比如节点 B 的任期编号为 3，节点 C 的任期编号是 4，节点 B 的最后一条日志项对应的任期编号为 3，而节点 C 为 2，那么当节点 C 请求节点 B 投票给自己时，节点 B 将拒绝投票

你可以把 Raft 的日志复制理解成一个优化后的二阶段提交（将二阶段优化成了一阶段），减少了一半的往返消息，也就是降低了一半的消息延迟。

raft中在脑裂场景出现两个leader时，客户端侧角度看是怎样的，有两个leader，会不会困惑，该怎么写？

raft协议在集群扩容时为什么要采用单节点变更策略。

Raft 集群具有容错能力，能容忍少数的节点故障，那么在多个 Raft 集群组成的 KV 系统中，如何设计一致哈希，实现当某个集群的领导者节点出现故障，并选举出新的领导者后，整个系统还能稳定运行呢？欢迎在留言区分享你的看法，与我一同讨论。

如何推导的？
我想说的是，这个算法有个非常致命的缺陷。如果将军数为 n、叛将数为 f，那么算法需要递归协商 f+1 轮，消息复杂度为 O(n ^ (f + 1))，消息数量指数级暴增。你可以想象一下，如果叛将数为 64，消息数已经远远超过 int64 所能表示的了，这是无法想象的，肯定不行啊。


ZAB 协议的最核心设计目标（如何实现操作的顺序性）
Paxos 虽然能保证达成共识后的值不再改变，但它不关心达成共识的值是什么。”


Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；
Multi-Paxos 思想，描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。

针对 Basic Paxos 的局限性 Multi-Paxos 又是如何改进的。

